<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, shrink-to-fit=no, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Simple Sidebar - Start Bootstrap Template</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/simple-sidebar.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body>

    <div id="wrapper">

        <!-- Sidebar -->
        <div id="sidebar-wrapper">
            <ul class="sidebar-nav">
                <li class="sidebar-brand">
                    <a href="#">
                        Khatereh Mohajery

                        Projects
                    </a>
                </li>
                <li>
                    <a href="#">Overview</a>
                </li>

                <li>
                    <a href="#">About</a>
                </li>
                <li>
                    <a href="#">Contact</a>
                </li>
            </ul>
        </div>
        <!-- /#sidebar-wrapper -->

        <!-- Page Content -->
        <div id="page-content-wrapper">
            <div class="container-fluid">
                <div class="row">
                    <div class="col-lg-12">
                        <h1>Predicting the Behavior of Financial Markets from News Headlines</h1>
                        <img src="images/Capstone_word1.png"/>
                        <p>   Wordcloud of most frequent words in news headlines<p>
                        <h2> Introduction </h2>
                        <p> Behavioral Economics believe in importance of emotions in human decision making process and subsequently its influence on market through investors and social mood. In recent years, variety of methods have been investigated to compute indicators of the public’s sentiment and mood state from large-scale online data instead of traditional surveys.One of the online data sources is news media content that has been shown to be an important factor shaping investor sentiment. In this project, the relation between news headline and market was investigated to explore the possibility of predicting market trends from news using natural language processing (NLP) techniques.<p>
                        <h2> Data </h2>
                        <p> Three measures were selected as market indicators including Dow Jones Industrial Average  (DJIA), the S&P 500 Volatility Index (VIX) and gold price. The data for these indicators was collected from Yahoo Finance website and <a href="http://www.gold.org/">gold.org</a> website for the years of 2008 to 2016. Two news headline datasets were employed in this test. The archive of New York Times website was scraped to collect the top business news headlines for each day from January 2016 to January 2017. More comprehensive dataset was downloaded from kaggle which included top 25 business headlines from reddit which was a collection of variety of sources from 2008 to 2016.</p>
                        <h2> Methods and Results </h2>
                        <p> Very weak correlation between the negative news sentiment score and market indicators was found. However, the sign of the correlations are in correct direction meaning that increased negativity score resulted in higher gold price and volatility index and lower DJ score all indicating negative trends in financial markets. The volatility index (VIX) showed highest correlation among market indicators and therefore used for predictions.<p>
                        <p> The other approach was to turn this problem to a classification problem. Each day that had a closing index higher than or equal to the opening index classified as a positive day (labeled as class 1). For those days with lower closing index, the day labeled as class zero. In this approach, all the words in news headlines for that day were used for the classification.<p>
                        <p> Using the headlines from the previous day in addition to the same day news headlines along with including two-word phrases increased the accuracy of the random forest classification model to 0.69 (from baseline accuracy of 0.53).<p>
                        <p> For more detail please check out <a href="https://github.com/KhaterehMohajery/Portfolio/blob/gh-pages/Portfolio_Piece_5.ipynb">here</a>. The code is also available at <a href="https://github.com/KhaterehMohajery/Portfolio/blob/gh-pages/Portfolio_Piece_5_notebook.ipynb">here</a>. <p>

                        <h1>Predicting the Presence of West Nile Virus in Chicago 2007-2014</h1>
                        <img src="images/kaggle_heatmap.png"/>
                        <p>   Heatmap of positions with higher probability of West Nile Virus presence<p>
                        <h2> Introduction </h2>
                        <p> West Nile virus is most commonly spread to humans through infected mosquitoes.
Traps has been set by Chicago Department of Public Health (CDPH) across the city of Chicago to capture mosquitoes and then tested for the virus. The results of these tests influence when and where the city will spray airborne pesticides to control adult mosquito populations.
Given weather, location and spraying data, this competition asked to predict when and where different species of mosquitos will test positive for West Nile virus. This will help the City of Chicago to more effectively allocate resources towards preventing transmission of this potentially deadly virus.<p>
                        <h2> Data and Methods</h2>
                        <p> Training dataset included the location, dates, species, number of mosquitoes and the presence of virus for each trap for years of 2007, 2009, 2011 and 2013.
The test dataset included above information for years 2008, 2010, 2012 and 2014 except for the type of mosquitoes found in traps. The spraying data was not provided for the test dataset and therefore could not be used as a predictor.
The weather conditions were averaged over 30 days and along with trap locations, dates and number of mosquitoes in traps were used as predictors in several classifying models (random forest, KNN, logistic regression and gradient boosting). For each model, best parameters were chosen using grid search. Due to inbalance classes (having a lot more incidences of 0 with no presence of the virus compared to 1), AUC metric was used as the classifier score.<p>
                        <h2> Results </h2>
                        <p> Stacking the results of two best classifying models of Gradient boosting and KNN gave the highest score of 0.85 on training set and 0.77376 on the submitted test set showing the trap being the most important predicting feature. Also, weather conditions such as Dew Point turned out to be important features.<p>

                        <p> Submitting the probabilities for the test dataset resulted in score of 0.77376. This ranked our group at 324 on Kaggle competition board which is among the top 20% of the competitors.<p>
                        <p> For more detail please check out <a href="https://github.com/KhaterehMohajery/Portfolio/blob/gh-pages/Portfolio_Piece_2.ipynb">here</a>. The code is also available at <a href="https://github.com/KhaterehMohajery/Portfolio/blob/gh-pages/Portfolio_Piece_2_notebook.ipynb">here</a>. <p>
                          <h1>Predicting Terrorism / Bayesian Inference </h1>
                          <img src="images/GTD_trend.png"/>
                          <p>   Trends for the number of terror attacks around the world from 1970 to 2015<p>
                          <h2> Introduction </h2>
                          <p> The aim of this project was to investigate some aspects of the Global Terrorism Database (GTD) with focusing on middle east region and ‘Bombing/Explosion’ terror attacks.<p>
                          <h2> Data and Methods</h2>
                          <p> The Global Terrorism Database was put together and is maintained by National Consortium for the Study of Terrorism and Responses to Terrorism (START). More than 150,000 terrorist incidents are recorded around the world from 1970 to 2015. The code book that comes with the data base explains the variables and information accompanying each incident. 137 variables describe information about time, type, location, motive and the number of people involved in each incident among many others. Unfortunately, the data for the year of 1993 is lost. <p>
                          <p> The Bayesian approach was taken to investigate whether the number of terror attacks happened in Lebanon and West Bank during years 2000 - 2015 are significantly different from each other or they are following a same distribution. The prior distribution was chosen to have a gamma distribution. Uniform distribution was assumed for the mean and standard deviation of prior. The mean and standard deviation of the data from years 1970 to 2000 for these two countries were set as the prior’s mean and standard deviation upper limit and zero as their lower limits.<p>
                          <p> In terms of predicting the bombing attacks in 1993 in Lebanon, it was assumed that the number of bombing attacks is linearly related to the year meaning: Number of bombing in Lebanon = B0 + B1 * (year-1970) Assuming uniform distribution for B0 (intercept) and Normal distribution for B1, the data from bombing attacks in Lebanon from 1970-2015 except for 1993 where the data was missing was used in the analysis. Poisson distribution was chosen as the prior distribution.<p>
                          <h2> Results </h2>
                          <p> The results for the number of terror attacks in Lebanon and West Bank during years 2000-2015 showed they are not statistically different. Lebanon has an average of 57 attacks with standard deviation of 79. West Bank has the average of 77 attacks with standard deviation of 83. However, since the 95% credible intervals for West Bank includes the mean value of Lebanon and also the difference of mean includes zero, the two distributions are not significantly different. <p>
                          <p> The bayesian regression resulted in very small B1 indicating the year does not linearly correlated with the number of bombing attacks in Lebanon and the intercept value which is a constant value takes the value of the average of bombings during the timeframe. The result of the prediction of bombing attacks in Lebanon in 1993 is basically the mean of the values of bombings in Lebanon between 1970-2015.<p>
                          <p> For more detail please check out <a href="https://github.com/KhaterehMohajery/Portfolio/blob/gh-pages/Portfolio_Piece_4.ipynb">here</a>. The code is also available at <a href="https://github.com/KhaterehMohajery/Portfolio/blob/gh-pages/Portfolio_Piece_4_notebook.ipynb">here</a>. <p>

                            <h1>SAT Scores and Trends Across US in 2001 </h1>
                            <img src="images/SAT_Verbal.png"/>
                            <p>   SAT Verbal Scores Across US in 2001 (blue indicates states with higher scores and red indicates states with lower average scores)<p>
                            <h2> Introduction </h2>
                            <p> This is just a quick investigation into SAT scores and rate of participation in SAT test across all states in USA in 2001 along with Tableau visualization. <p>
                                <h2> Data </h2>
                            <p>The dataset has the inforamation on the verbal and math average scores along with the participation rate in the SAT exam in all 50 states plus District of Columbia in 2001.However, according to <a href="http://nces.ed.gov/programs/digest/d14/tables/dt14_226.40.asp?current=yes">this</a> the Math score for Ohio State should be corrected from 439 to 539. <p>
                              <h2> Results </h2>
                            <p> It was expected to have normally distributed histograms for scores. However,  histograms showed bimodal distributions suggesting that two distinct populations are present in the dataset. The inverse correlation of scores and participation rates also helped with understanding of what is going on. Separating the states based on whether the SAT test is mandatory or not, matched very well with high participation rates and low participation rates. This separation indicates that in states where the test is mandatory the participation rates are higher. However, since all students take the test the average scores are lower. In states where the test is not mandatory the participation rates are lower but because only high achiever and motivated students take the test to have the option to go to the best colleges anywhere in US, the scores are higher.<p>

                              <p> For more detail please check out <a href="https://github.com/KhaterehMohajery/Portfolio/blob/gh-pages/Portfolio_Piece_1_notebook.ipynb">here</a>.<p>

                    </div>
                </div>
            </div>
        </div>
        <!-- /#page-content-wrapper -->

    </div>
    <!-- /#wrapper -->

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Menu Toggle Script -->
    <script>
    $("#menu-toggle").click(function(e) {
        e.preventDefault();
        $("#wrapper").toggleClass("toggled");
    });
    </script>

</body>

</html>
